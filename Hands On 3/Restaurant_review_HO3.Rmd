---
title: "Restaurant_review_HO3"
author: "Bruno DÃ­ez Buitrago"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

The goal of this document is to show a sample script for pattern-based entity recognition over text documents using the openNLP (natural language processing) and the tm (text mining) packages in R  
This is part of the Intelligent Systems project found on https://github.com/brunaldo07/Hands-On-Project, precisely the third hands on 

```{r include=FALSE}
#Please set your corresponding path to the directory in your terminal
setwd("F:/informatica/6 Master Data Science/Intelligent Systems/Unit 5/Hands-On-Project/Hands On 2")

#Setting up mirror
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

#Including libraries
# Needed for OutOfMemoryError: Java heap space 
library(rJava)
.jinit(parameters="-Xmx4g")
# If there are more memory problems, invoke gc() after the POS tagging

library(NLP)
install.packages("openNLP")
library(openNLP) 
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
library(tm)
library(stringr)
library(ggplot2)
install.packages("ggtext")
library(ggtext)
options(java.parameters = "-Xmx8000m")
```


# Auxiliary functions

getAnnotationsFromDocument returns annotations for the text document: word, sentence, part-of-speech, and Penn Treebank parse annotations.

As an alternative, the koRpus package uses TreeTagger for POS tagging.

```{r include = F}
getAnnotationsFromDocument = function(doc){
  x=as.String(doc)
  sent_token_annotator <- Maxent_Sent_Token_Annotator()
  word_token_annotator <- Maxent_Word_Token_Annotator()
  pos_tag_annotator <- Maxent_POS_Tag_Annotator()
  y1 <- annotate(x, list(sent_token_annotator, word_token_annotator))
  y2 <- annotate(x, pos_tag_annotator, y1)
  return(y2)  
} 
```

getAnnotatedMergedDocument returns the text document merged with the annotations.

```{r include = F}
getAnnotatedMergedDocument = function(doc,annotations){
  x=as.String(doc)
  y2w <- subset(annotations, type == "word")
  tags <- sapply(y2w$features, '[[', "POS")
  r1 <- sprintf("%s/%s", x[y2w], tags)
  r2 <- paste(r1, collapse = " ")
  return(r2)  
} 
```

getAnnotatedPlainTextDocument returns the text document along with its annotations in an AnnotatedPlainTextDocument.

```{r include = F}
getAnnotatedPlainTextDocument = function(doc,annotations){
  x=as.String(doc)
  a = AnnotatedPlainTextDocument(x,annotations)
  return(a)  
} 
```

detectPatternOnDocument returns the pattern detected on an AnnotatedPlainTextDocument.
```{r include = F}
detectPatternOnDocument <- function(doc, pattern) {
  x=as.String(doc)
  res=str_match_all(x,pattern)
  
  dimrow=dim(res[[1]])[1]
  dimcol=dim(res[[1]])[2]
  
  # If there are no rows, no matches have been found
  if (dimrow == 0) {
    return(NA)
  }else{
    if (dimcol > 2){
      # If there are three or more columns, we have to paste all the groups together
      for (i in 1:dimrow) {
        res[[1]][i,2] = paste(res[[1]][i,2:dimcol], collapse = ' ')
      }
    }
    
    # We return all the results found separated by ','
    if (dimcol != 1) {
      result = paste(res[[1]][,2], collapse = ', ')
    }else{
      result = paste(res[[1]][,1], collapse = ', ')
    }
    return(result)
  }
}
```

detectPatternOnDocumentWithContext returns the pattern detected on an AnnotatedPlainTextDocument with some context.

```{r include = F}
detectPatternOnDocumentWithContext <- function(doc, pattern) {
  txt=as.String(doc)
  number=50
  coord=str_locate(txt,pattern)
  res3=substr(txt,coord[1]-number,coord[2]+number)
  return (res3)
}
```

detectPatternsInCorpus returns a data frame with all the patterns detected in a corpus.

```{r include = F}
detectPatternsInCorpus = function(corpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(corpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
    }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
    }
  return (vallEntities)  
  }
```


detectPatternsInTaggedCorpus returns a data frame with all the patterns detected in an annotated corpus.

```{r include = F}
detectPatternsInTaggedCorpus = function(corpus, taggedCorpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(taggedCorpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
    }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
    }
  return (vallEntities)  
  }
```

countMatchesPerColumn returns the number of matches per pattern/column.
Counts the number of columns with non-NA values for each pattern.
```{r include = F}
countMatchesPerColumn = function (df) {
  entityCountPerPattern <- data.frame(matrix(NA, ncol = 2, 
                                             nrow = length(names(df))-1))
  names(entityCountPerPattern) <- c("Entity","Count")
  
  for (i in 2:length(names(df))) {
    entityCountPerPattern$Entity[i-1] = names(df)[i]
    entityCountPerPattern$Count[i-1] = nrow(subset(df, !is.na(df[i])))
    }
  return (entityCountPerPattern)
  }
```

countMatchesPerRow returns the number of entities per file/row.
Counts the number of rows with non-NA values for each file.
```{r}
countMatchesPerRow = function (df) {
  entityCountPerFile <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(entityCountPerFile) <- c("File","Count")
  
  for (i in 1:nrow(df)) {
    entityCountPerFile$File[i] = df$File[i]
    entityCountPerFile$Count[i] = length(Filter(Negate(is.na),df[i,2:length(df[i,])]))
    }
  return (entityCountPerFile[entityCountPerFile[2]!=0,])
  }
```

printMatchesPerPattern prints the matches found per pattern.

```{r include = F}
printMatchesPerPattern = function (patterns, matches) {
  for (i in 1:length(patterns)){
    print(paste("PATTERN: ",patterns[i]))
    strings = matches[,i+1][!is.na(unlist(matches[,i+1]))]
    print(strings)
    print(" ") 
  }
}
```

mergeAllMatchesInLists returns a data frame with all the files and their matches in a single list per file.

```{r include=F}
mergeAllMatchesInLists = function (df) {
  matchesPerFile = rep(list(list()), nrow(df))
  for (i in 1:nrow(df)) {    
    matches=list()
    for (j in 2:ncol(df)){
      if (grepl(',',df[i,j])){
        b=strsplit(as.character(df[i,j]),split=',')
        for (j in 1:length(b[[1]])){
          matches= c(matches,str_trim(b[[1]][j]))
        }
      }else{
        if (!(is.na(df[i,j]))){
          matches = c(matches,str_trim(df[i,j]))
        }
      }
    }
    matches = unique(matches)
    matchesPerFile[[i]]=append(matchesPerFile[[i]],matches)
  }
  
  files = df[,1]
  matches = matchesPerFile
  
  allMatches<- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(allMatches) <- c("Files","Matches")
  
  allMatches$Files=files
  allMatches$Matches=matches
  
  return (allMatches)
}
```

#Loading corpus

We are using a reduced version of the dataset found in https://www.kaggle.com/vigneshwarsofficial/reviews  containing information about restaurant reviews. 
In this hand on, the 1000 reviews set is too big to be processed in a reasonable amount of time so we are using a set of 100 reviews placed on "Resources/short". 

```{r}
source.pos = DirSource("../Resources/short", encoding = "UTF-8")
corpus = Corpus(source.pos)
```


# Inspecting the corpus


Let's see the length of the corpus and a the first entries to see the kind of documents it includes

```{r }
length(corpus)
summary(corpus[1:10])

```

# Finding patterns

## Simple patterns
We are going to define some simple patterns to detect expressions clients use in their reviews
```{r}
# pattern0=c("good")
# pattern0=c(pattern0,"like")
# pattern0=c(pattern0,"back")
# pattern0=c(pattern0,"servic")
# pattern0=c(pattern0,"place")
# pattern0=c(pattern0,"time[s]?")
pattern0=c("was")
pattern0=c(pattern0,"this")
pattern0=c(pattern0,"be")
pattern0=c(pattern0,"the")
pattern0=c(pattern0,"worst")
pattern0=c(pattern0,"place")

```

Let's see where we can find these patterns in the corpus

```{r}
matches0 = detectPatternsInCorpus(corpus, pattern0)
matches0
matches0[!is.na(matches0[2]),c(1,2)]
matches0[!is.na(matches0[3]),c(1,3)]
matches0[!is.na(matches0[4]),c(1,4)]
matches0[!is.na(matches0[5]),c(1,5)]
matches0[!is.na(matches0[6]),c(1,6)]
matches0[!is.na(matches0[7]),c(1,7)]


```

Let's see how many matches we find in each file, and how many times each pattern is found

```{r}
countMatchesPerRow(matches0) 
countMatchesPerColumn(matches0) 

```

Print of the context where the patterns are found
```{r}
for (i in 1:length(pattern0)){
  print(paste("PATTERN: ",pattern0[i]))
  strings = lapply(corpus, detectPatternOnDocumentWithContext, pattern=pattern0[i])
  print(unlist(strings[!is.na(unlist(strings))]))
  print(" ")
}
```
## Regular expressions
Let's try and use some more complex regular expressions
```{r}
pattern1=c("was [A-z]*")
pattern1=c(pattern1,"this [A-z]*")
pattern1=c(pattern1,"be [A-z]*")
pattern1=c(pattern1,"the [A-z]* [A-z]*")
pattern1=c(pattern1,"[A-z]* time[s]?")
pattern1=c(pattern1,"[A-z]* place[s]?")

```

We see where they appear
```{r}
matches1 = detectPatternsInCorpus(corpus, pattern1)
matches1[!is.na(matches1[2]),c(1,2)]
matches1[!is.na(matches1[3]),c(1,3)]
matches1[!is.na(matches1[4]),c(1,4)]
matches1[!is.na(matches1[5]),c(1,5)]
matches1[!is.na(matches1[6]),c(1,6)]
matches1[!is.na(matches1[7]),c(1,7)]

```
We see the matches per pattern

```{r}
printMatchesPerPattern(pattern1, matches1)
```

We can check the matches found per file and the number of times the patterns has been found
```{r}
countMatchesPerRow(matches1) 
countMatchesPerColumn(matches1) 

```
## POS tags
We include part of speech tagas in our pattterns to refine the search
```{r}
pattern2=c("was/VBD [A-z]*")
pattern2=c(pattern2,"this/DT [A-z]*")
pattern2=c(pattern2,"be/VB [A-z]*")
pattern2=c(pattern2,"the/DT [A-z]* [A-z]*")
pattern2=c(pattern2,"[A-z]* time[s]?/NN[S]?")
pattern2=c(pattern2,"[A-z]* place[s]?/NN[S]?")

```

Let's detect these new patterns
```{r}
allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allEntities[!is.na(allEntities[2]),c(1,2)]
allEntities[!is.na(allEntities[3]),c(1,3)]
allEntities[!is.na(allEntities[4]),c(1,4)]
allEntities[!is.na(allEntities[5]),c(1,5)]
allEntities[!is.na(allEntities[6]),c(1,6)]
allEntities[!is.na(allEntities[7]),c(1,7)]


```

These are the found entities for each pattern
```{r}
Filter(Negate(is.na),allEntities[[2]])
Filter(Negate(is.na),allEntities[[3]])
Filter(Negate(is.na),allEntities[[4]])
Filter(Negate(is.na),allEntities[[5]])
Filter(Negate(is.na),allEntities[[6]])
Filter(Negate(is.na),allEntities[[7]])
printMatchesPerPattern(pattern2, allEntities)


```

Lastly, we can plot a histogram of the frequencies of the matched patterns
```{r}
entityCountPerPattern = countMatchesPerColumn(allEntities)
entityCountPerPattern
hist(entityCountPerPattern$Count)

```

Or counting the matches per file
```{r}
entityCountPerFile=countMatchesPerRow(allEntities)
entityCountPerFile
hist(entityCountPerFile$Count)

```
To end hese results will be stored in a csv file 
```{r}
write.table(allEntities, file = "restaurant_reviews_entities.csv", row.names = F, na="", sep=";")

```

I could not manage to resolve a gold standard for this dataset, so we can not make any comparisons with it sadly.
